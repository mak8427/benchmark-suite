name: Tests & Quality
on:
  # Run on every commit to any branch
  push:
  pull_request:
  schedule:
    # Run dependency updates weekly
    - cron: '0 2 * * 1'

env:
  PYTHON_VERSION: "3.12"

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r src/requirements.txt
          pip install pytest-cov pytest-xdist

      - name: Install pre-commit hooks
        run: |
          pip install pre-commit

      - name: Run linting (warn-only)
        continue-on-error: true
        run: pre-commit run --all-files --show-diff-on-failure


      - name: Run unit tests with coverage
        run: |
          python -m pytest tests/ -v --tb=short --cov=src --cov-report=xml --cov-report=term-missing



  cli-slurm:
    runs-on: ubuntu-latest
    services:
      mysql:
        image: mysql:8.0
        env:
          MYSQL_ROOT_PASSWORD: root
        ports:
          - "8888:3306"
        options: --health-cmd="mysqladmin ping" --health-interval=10s --health-timeout=5s --health-retries=3
    steps:
      - uses: actions/checkout@v4

      - uses: koesterlab/setup-slurm-action@v1
      - name: Smoke test
        run: squeue && sinfo && sbatch --version

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r src/requirements.txt
          pip install build twine pytest

      - name: Build package
        run: |
          pip install -e .

      - name: list
        run: |
          benchwrap list

      - name: Create test SLURM job
        run: |
          # Create a simple test benchmark for CI
          mkdir -p /tmp/slurm_test
          cat > /tmp/slurm_test/simple_benchmark.py << 'EOF'
          #!/usr/bin/env python3
          import time
          import os
          
          print("Starting simple benchmark test...")
          print(f"Job ID: {os.environ.get('SLURM_JOB_ID', 'N/A')}")
          print(f"Node list: {os.environ.get('SLURM_JOB_NODELIST', 'N/A')}")
          
          # Simple computation to simulate work
          result = sum(i**2 for i in range(1000))
          print(f"Computation result: {result}")
          
          # Simulate some processing time
          time.sleep(2)
          
          print("Benchmark test completed successfully")
          EOF
          
          chmod +x /tmp/slurm_test/simple_benchmark.py

      - name: Test SLURM job submission and execution
        run: |
          # Create a simple SLURM script for testing
          cat > /tmp/test_job.sh << 'EOF'
          #!/bin/bash
          #SBATCH --job-name=ci-test
          #SBATCH --nodes=1
          #SBATCH --ntasks=1
          #SBATCH --cpus-per-task=1
          #SBATCH --time=00:02:00
          #SBATCH --output=/tmp/slurm_output_%j.out
          #SBATCH --error=/tmp/slurm_error_%j.err
          
          echo "SLURM job started at $(date)"
          echo "Job ID: $SLURM_JOB_ID"
          echo "Node: $SLURM_JOB_NODELIST"
          
          python3 /tmp/slurm_test/simple_benchmark.py
          
          echo "SLURM job completed at $(date)"
          EOF
          
          chmod +x /tmp/test_job.sh
          
          # Submit the job and capture job ID
          echo "Submitting SLURM test job..."
          JOB_ID=$(sbatch --parsable /tmp/test_job.sh)
          echo "Submitted job with ID: $JOB_ID"
          
          # Wait for job to complete (with timeout)
          echo "Waiting for job to complete..."
          timeout=60
          elapsed=0
          while [ $elapsed -lt $timeout ]; do
            if squeue -j $JOB_ID 2>/dev/null | grep -q $JOB_ID; then
              echo "Job $JOB_ID is still running..."
              sleep 5
              elapsed=$((elapsed + 5))
            else
              echo "Job $JOB_ID has completed"
              break
            fi
          done
          
          if [ $elapsed -ge $timeout ]; then
            echo "ERROR: Job did not complete within timeout"
            squeue -j $JOB_ID || echo "Job not found in queue"
            exit 1
          fi
          
          # Check job status
          echo "Checking job status..."
          sacct -j $JOB_ID --format=JobID,JobName,State,ExitCode,Elapsed,CPUTime || echo "sacct failed, continuing..."
          
          # Verify output files exist and contain expected content
          if [ -f "/tmp/slurm_output_${JOB_ID}.out" ]; then
            echo "=== Job Output ==="
            cat "/tmp/slurm_output_${JOB_ID}.out"
          else
            echo "ERROR: Output file not found"
            exit 1
          fi
          
          if [ -f "/tmp/slurm_error_${JOB_ID}.err" ]; then
            echo "=== Job Errors ==="
            cat "/tmp/slurm_error_${JOB_ID}.err"
          fi
          
          # Verify the output contains expected content
          if grep -q "Benchmark test completed successfully" "/tmp/slurm_output_${JOB_ID}.out"; then
            echo "✓ SLURM job executed successfully and produced expected output"
          else
            echo "ERROR: Expected output not found in job output"
            exit 1
          fi

      - name: Test benchmark suite SLURM integration
        run: |
          # Test that the benchmark suite can handle SLURM environments
          echo "Testing benchmark suite SLURM detection..."
          
          # Test listing benchmarks
          benchwrap list
          
          # Run SLURM-specific pytest tests
          echo "Running SLURM integration tests..."
          python -m pytest tests/test_slurm.py -v --tb=short
          
          echo "✓ SLURM integration tests completed"

      - name: Test flops_matrix_mul_mini benchmark with SLURM
        run: |
          # Test the specific flops_matrix_mul_mini benchmark
          echo "Testing flops_matrix_mul_mini benchmark..."
          
          # Create a lightweight version of the benchmark for CI testing
          mkdir -p /tmp/benchwrap_ci_test
          cat > /tmp/benchwrap_ci_test/mini_flops_test.sh << 'EOF'
          #!/bin/bash
          #SBATCH --job-name=flops-mini-ci
          #SBATCH --nodes=1
          #SBATCH --ntasks=1
          #SBATCH --cpus-per-task=1
          #SBATCH --time=00:03:00
          #SBATCH --output=/tmp/flops_output_%j.out
          #SBATCH --error=/tmp/flops_error_%j.err
          
          echo "Starting flops_matrix_mul_mini CI test at $(date)"
          echo "Job ID: $SLURM_JOB_ID"
          echo "Node: $SLURM_JOB_NODELIST"
          echo "CPU cores: $SLURM_CPUS_PER_TASK"
          
          # Run a small version of the matrix multiplication test
          python3 -c "
          import numpy as np
          import time
          import os
          
          print('Starting mini FLOPS test...')
          n = 500  # Much smaller than the 12,000 in the real benchmark
          print(f'Creating {n}x{n} matrices for CI test')
          
          a = np.random.random((n, n))
          b = np.random.random((n, n))
          
          print(f'Multiplying on {os.cpu_count()} logical cores...')
          start = time.time()
          c = a @ b
          duration = time.time() - start
          
          flops = 2 * n**3 / duration / 1e9
          norm = np.linalg.norm(c)
          
          print(f'GEMM completed in {duration:.3f}s → {flops:.2f} GFLOP/s')
          print(f'Result Frobenius norm: {norm:.3e}')
          print('Mini FLOPS test completed successfully')
          "
          
          echo "flops_matrix_mul_mini CI test completed at $(date)"
          EOF
          
          chmod +x /tmp/benchwrap_ci_test/mini_flops_test.sh
          
          # Submit the benchmark test job
          echo "Submitting flops_matrix_mul_mini test job..."
          FLOPS_JOB_ID=$(sbatch --parsable /tmp/benchwrap_ci_test/mini_flops_test.sh)
          echo "Submitted flops benchmark job with ID: $FLOPS_JOB_ID"
          
          # Monitor job status
          echo "Monitoring flops benchmark job..."
          timeout=120  # 2 minutes should be enough
          elapsed=0
          while [ $elapsed -lt $timeout ]; do
            if squeue -j $FLOPS_JOB_ID 2>/dev/null | grep -q $FLOPS_JOB_ID; then
              echo "Flops job $FLOPS_JOB_ID is still running... (${elapsed}s elapsed)"
              sleep 10
              elapsed=$((elapsed + 10))
            else
              echo "Flops job $FLOPS_JOB_ID has completed"
              break
            fi
          done
          
          if [ $elapsed -ge $timeout ]; then
            echo "ERROR: Flops job did not complete within timeout"
            squeue -j $FLOPS_JOB_ID || echo "Job not found in queue"
            exit 1
          fi
          
          # Check job completion status
          echo "Checking flops job completion..."
          sacct -j $FLOPS_JOB_ID --format=JobID,JobName,State,ExitCode,Elapsed || echo "sacct not available"
          
          # Verify output files and content
          if [ -f "/tmp/flops_output_${FLOPS_JOB_ID}.out" ]; then
            echo "=== Flops Job Output ==="
            cat "/tmp/flops_output_${FLOPS_JOB_ID}.out"
            echo "========================="
            
            # Verify expected output
            if grep -q "Mini FLOPS test completed successfully" "/tmp/flops_output_${FLOPS_JOB_ID}.out" && \
               grep -q "GFLOP/s" "/tmp/flops_output_${FLOPS_JOB_ID}.out"; then
              echo "✓ flops_matrix_mul_mini benchmark executed successfully"
            else
              echo "ERROR: Expected benchmark output not found"
              exit 1
            fi
          else
            echo "ERROR: Flops benchmark output file not found"
            ls -la /tmp/flops_*
            exit 1
          fi
          
          # Check for errors
          if [ -f "/tmp/flops_error_${FLOPS_JOB_ID}.err" ]; then
            error_content=$(cat "/tmp/flops_error_${FLOPS_JOB_ID}.err")
            if [ -n "$error_content" ]; then
              echo "=== Flops Job Errors ==="
              echo "$error_content"
              echo "========================"
              # Don't fail on warnings, only on actual errors
              if echo "$error_content" | grep -i "error\|exception\|traceback"; then
                echo "ERROR: Found errors in benchmark execution"
                exit 1
              fi
            fi
          fi
          
          echo "✓ flops_matrix_mul_mini benchmark test completed successfully"



